import numpy as np
import pandas as pd
import random

import obp
from obp.dataset import SyntheticBanditDataset


import os
import sys
if os.path.dirname(__file__) == '':
    sys.path.append('../../src')
else:
    sys.path.append(os.path.dirname(__file__) + '/../../src')
from data.modify_batch_feedback import section_sampling_from_batch_feedback, merge_batch_feedback


class MultipleBetaSyntheticData:
    """SyntheticBanditDataset with multiple beta. (Merge two batch data generated by different beta)
    """

    def __init__(
        self, 
        n_actions, 
        dim_context, 
        reward_type, 
        reward_function, 
        reward_std=1.0, 
        action_context=None,
        behavior_policy_function=None, 
        beta=[2.0, 4.0], 
        random_state=1, 
        dataset_name='synthetic_bandit_dataset'
        ):
        """set basic info

        Args:
            n_actions (int): number of actions
            dim_context (int): dim of context
            reward_type (str): binary or continuous
            reward_function (predictor-class): predictor class
            reward_std (float, optional): Defaults to 1.0.
            action_context (np.array, optional): Defaults to None.
            behavior_policy_function (Callable[[np.ndarray, np.ndarray], np.ndarray], optional): Defaults to None.
            beta (list, optional): Defaults to [2.0, 4.0]. List of beta.
            random_state (int, optional): Defaults to 1.
            dataset_name (str, optional): Defaults to 'synthetic_bandit_dataset'.
        """

        assert (type(beta) == list) or (type(beta) == float), 'type(beta) must be list or float'
        if type(beta) == list:
            assert len(beta) == 2, 'len(beta) must be 2 if type(beta) == list'

        self.n_actions = n_actions
        self.dim_context = dim_context
        self.reward_type = reward_type
        self.reward_function = reward_function
        self.reward_std = reward_std
        self.action_context = action_context
        self.behavior_policy_function = behavior_policy_function

        self.beta = beta

        if random_state is None:
            self.random_state = random.randint(1,10000000)
        else:
            self.random_state = random_state

        self.dataset_name = dataset_name

        self.obtain_count = 1



    def obtain_batch_bandit_feedback(self, n_rounds):
        """get batch bandit feedback

        Args:
            n_rounds (int): sample size

        Returns:
            dict: batch bandit feedback
        """

        if type(self.beta) == list:
            self.beta_1 = self.beta[0]
            self.beta_2 = self.beta[1]
        else:
            self.beta_1 = float(self.beta)
            self.beta_2 = float(self.beta)

        self.data_1 = SyntheticBanditDataset(
            n_actions=self.n_actions,
            dim_context=self.dim_context,
            beta=self.beta_1,  
            reward_type=self.reward_type, 
            reward_function=self.reward_function, 
            reward_std=self.reward_std, 
            behavior_policy_function=self.behavior_policy_function,
            action_context=self.action_context, 
            random_state=self.random_state,
            dataset_name=self.dataset_name
            )
        self.data_2 = SyntheticBanditDataset(
            n_actions=self.n_actions,
            dim_context=self.dim_context,
            beta=self.beta_2,  
            reward_type=self.reward_type, 
            reward_function=self.reward_function, 
            reward_std=self.reward_std, 
            behavior_policy_function=self.behavior_policy_function,
            action_context=self.action_context, 
            random_state=self.random_state,
            dataset_name=self.dataset_name
            )

        for count in range(self.obtain_count):
            if count == self.obtain_count-1:
                all_batch_1 = self.data_1.obtain_batch_bandit_feedback(n_rounds=n_rounds)
                all_batch_2 = self.data_2.obtain_batch_bandit_feedback(n_rounds=n_rounds)
            else:
                self.data_1.obtain_batch_bandit_feedback(n_rounds=n_rounds)
                self.data_2.obtain_batch_bandit_feedback(n_rounds=n_rounds)
                
        n_rounds_half = int(n_rounds / 2)
        batch_1_first_half = section_sampling_from_batch_feedback(batch_bandit_feedback=all_batch_1, section_start=0, section_end=n_rounds_half)
        batch_1_second_half = section_sampling_from_batch_feedback(batch_bandit_feedback=all_batch_1, section_start=n_rounds_half, section_end=n_rounds)
        batch_2_first_half = section_sampling_from_batch_feedback(batch_bandit_feedback=all_batch_2, section_start=0, section_end=n_rounds_half)
        batch_2_second_half = section_sampling_from_batch_feedback(batch_bandit_feedback=all_batch_2, section_start=n_rounds_half, section_end=n_rounds)

        merged_batch = merge_batch_feedback(
            batch_bandit_feedback_1=batch_1_first_half, 
            action_dist_1_by_2=batch_2_first_half['pi_b'], 
            batch_bandit_feedback_2=batch_2_second_half, 
            action_dist_2_by_1=batch_1_second_half['pi_b'], 
            )
        
        self.obtain_count += 1
        
        return merged_batch


    def calc_ground_truth_policy_value(self, expected_reward, action_dist):
        """get expected policy valu

        Args:
            expected_reward (np.array): array of expected reward
            action_dist (np.array): action distribution

        Returns:
            float: ground truth of policy value
        """

        return np.average(expected_reward, weights=action_dist[:, :, 0], axis=1).mean()



